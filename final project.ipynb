{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a919f2-77d5-44d4-9967-19ee12f2952b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718451f8-e4fe-4602-832c-2fd899d407e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6951d21e-eca2-40c7-a8e5-879c5bdf0557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:03:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Read lines from a file stream\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c56f2c2-2e28-4ad7-aead-2112ef9d95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d69b63-e0f6-4e30-9e00-9feb39ae18b0",
   "metadata": {},
   "source": [
    "#### Define schemas one for each folder of the provided data VertFinalExam and EdgesFinalExam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1742a54-1ec7-485b-ad26-a9de50e93504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa69ae1-bcf0-4115-98ca-9ff2eaf48f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertSchema = StructType([StructField('id', StringType(), True),\n",
    "                           StructField('City', StringType(), True),\n",
    "                           StructField('State', StringType(), True),\n",
    "                           StructField('Country', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940725f1-390e-42b6-9cb4-4522e11fb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgSchema = StructType([StructField('tripid', IntegerType(), True),\n",
    "                        StructField('delay', IntegerType(), True),\n",
    "                        StructField('distance', IntegerType(), True),\n",
    "                        StructField('src', StringType(), True),\n",
    "                        StructField('dst', StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114f9ed-7eed-4067-bbaf-0d1afef2d25c",
   "metadata": {},
   "source": [
    "#### Create a streaming reader to read streaming data from the reading sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbdd21f9-0b2b-4512-ada5-95b88d8967f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_df = spark.readStream.format(\"parquet\")\\\n",
    ".schema(vertSchema)\\\n",
    ".load(\"/home/wick/Big_Data/final_project/VertFinalExam/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76cdba44-f9c9-4481-8467-4feee4aee7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vert_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcb79f24-ceac-4688-ada3-513602f4bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edg_df = spark.readStream.format(\"parquet\")\\\n",
    ".schema(edgSchema)\\\n",
    ".load(\"/home/wick/Big_Data/final_project/EdgesFinalExam/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48d7df98-6bb4-4704-8da5-1630912f0ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edg_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bff6c3-e00f-4794-adbe-cda342efc0ab",
   "metadata": {},
   "source": [
    "#### For the streaming Edges dataframe create a new column to indicate delay categories as follow:\n",
    "\n",
    "    Early: for early delays (-ve delay values).\n",
    "    Late: for delayed flights (+ve delay values).\n",
    "    OnTime: for on time flights (0 delay values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b94919-1de8-4211-ba66-1e63051c74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "edg_df1 = edg_df.withColumn(\n",
    "                              \"delay_category\",\n",
    "                              when(col(\"delay\") < 0, \"Early\")\n",
    "                             .when(col(\"delay\") > 0, \"Late\")\n",
    "                             .otherwise(\"OnTime\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2edf394e-3a36-481f-94c9-a5719f43f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- delay_category: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edg_df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92d54a-607e-4c21-a206-2e46496492a0",
   "metadata": {},
   "source": [
    "#### For the streaming Vertices dataframe remove all rows that contain state as an emplty string state=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7194795f-d746-42a9-85a1-bcdf4a68db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_df1 = vert_df.filter(col(\"State\") != \"\")\n",
    "#vert_df1 = vert_df.where(col(\"State\") != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56340f3b-5e43-4510-8560-c34f38d7755f",
   "metadata": {},
   "source": [
    "#### Create a writer for the final streaming Edges dataframe to write the streaming data in writing sink in a parquet fromat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128c933b-02f6-465a-9e9c-338cdf22cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edg_writer = edg_df1.writeStream.outputMode(\"append\")\\\n",
    "                .format(\"parquet\")\\\n",
    "                .option(\"path\", \"/home/wick/Big_Data/final_project/first_folder/\")\\\n",
    "                .option(\"checkpointLocation\", \"/home/wick/Big_Data/final_project/edg_chkpnt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01958e20-ec2a-47fe-8e30-1c194c5ebc55",
   "metadata": {},
   "source": [
    "#### Create a writer for the final streaming Vertices dataframe to write the streaming data in writing sink in a parquet fromat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a88104-1f66-441d-8165-672cd03d60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_writer = vert_df1.writeStream.outputMode(\"append\")\\\n",
    "                .format(\"parquet\")\\\n",
    "                .option(\"path\", \"/home/wick/Big_Data/final_project/second_folder/\")\\\n",
    "                .option(\"checkpointLocation\", \"/home/wick/Big_Data/final_project/vert_chkpnt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e141f1-6215-4bc2-84a3-b92047a5d6df",
   "metadata": {},
   "source": [
    "#### Start a query for the Edges writer. Copy and paste your EdgesFinalExam data to the edges streaming reading source. Wait to make sure the writing sink folder contains all data. Then stop the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2911d89-83cb-463b-b025-54ceaf153f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:04:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "edg_query = edg_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba7bc21e-7ad2-496e-b851-a0b34e7a090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edg_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4afa2-c874-4e0c-a81d-8f08342573f7",
   "metadata": {},
   "source": [
    "#### Start a query for the Vertices writer. Copy and paste your VertFinalExam data to the vertices streaming reading source. Wait to make sure the writing sink folder contains all data. Then stop the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fe2e293-e8bc-41bf-8225-367eea42d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:04:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "vert_query = vert_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d86675-a007-4581-9096-30e844b1d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875bfa9-ec66-433b-a666-47efaeeb4c0b",
   "metadata": {},
   "source": [
    "#### Using spark.read():\n",
    "\n",
    "    Read the vertices data from the writing sink directory into static Vertices dataframe.\n",
    "    Read the edges data from the writing sink directory into static Edges dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dbc89cf-8cf1-4867-b89b-7f44da88c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_verts = spark.read.format('parquet')\\\n",
    ".load('/home/wick/Big_Data/final_project/second_folder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b415f4b7-bc58-414c-b68e-d872443d2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edgs = spark.read.format('parquet')\\\n",
    ".load('/home/wick/Big_Data/final_project/first_folder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e80e0d6c-2cb5-4b30-a057-30e2e4ea3a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+-------+\n",
      "| id|       City|State|Country|\n",
      "+---+-----------+-----+-------+\n",
      "|ABE|  Allentown|   PA|    USA|\n",
      "|ABI|    Abilene|   TX|    USA|\n",
      "|ABQ|Albuquerque|   NM|    USA|\n",
      "|ABR|   Aberdeen|   SD|    USA|\n",
      "|ABY|     Albany|   GA|    USA|\n",
      "+---+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_verts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36e6edbe-64bd-4b18-a026-7ec6f071783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+---+---+--------------+\n",
      "| tripid|delay|distance|src|dst|delay_category|\n",
      "+-------+-----+--------+---+---+--------------+\n",
      "|1010630|  -10|     928|RSW|EWR|         Early|\n",
      "|1021029|   87|     974|RSW|ORD|          Late|\n",
      "|1021346|    0|     928|RSW|EWR|        OnTime|\n",
      "|1021044|   18|     928|RSW|EWR|          Late|\n",
      "|1021730|   29|     748|RSW|IAH|          Late|\n",
      "+-------+-----+--------+---+---+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_edgs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d02dca1-ee26-42d5-a7dc-c4778528250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- delay_category: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_edgs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd995de-73c4-4faa-8ba1-d1ba5a1e5adf",
   "metadata": {},
   "source": [
    "#### Create a GraphFrame from these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a9291f9-71f9-4892-9a20-492583f0f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f701ff55-4f75-41b2-a88b-531c51f5cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "GF = GraphFrame(df_verts, df_edgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a477f-030e-47f0-8fb6-2322091229bb",
   "metadata": {},
   "source": [
    "#### Apply PageRank algorithm to find the most 10 important Vertices. Order the results based on the rank in descending order. \n",
    "#### Use maxIter=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7939f9f7-3cf5-43f2-8e4b-e41bc1eb53c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----+-------+------------------+\n",
      "| id|          City|State|Country|          pagerank|\n",
      "+---+--------------+-----+-------+------------------+\n",
      "|ATL|       Atlanta|   GA|    USA|31.402169285067313|\n",
      "|DFW|        Dallas|   TX|    USA| 22.76415219751248|\n",
      "|ORD|       Chicago|   IL|    USA| 21.83241348762772|\n",
      "|DEN|        Denver|   CO|    USA|16.026921025779515|\n",
      "|LAX|   Los Angeles|   CA|    USA|14.358865452635795|\n",
      "|IAH|       Houston|   TX|    USA|13.229634347806075|\n",
      "|SFO| San Francisco|   CA|    USA|11.322517232690489|\n",
      "|PHX|       Phoenix|   AZ|    USA|10.852423159730376|\n",
      "|SLC|Salt Lake City|   UT|    USA| 9.622759351860472|\n",
      "|LAS|     Las Vegas|   NV|    USA| 8.778471071473987|\n",
      "+---+--------------+-----+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagerank = GF.pageRank(resetProbability=0.15, maxIter=5)\n",
    "\n",
    "vertices = pagerank.vertices\n",
    "\n",
    "top_10_vertices = vertices.orderBy(vertices.pagerank.desc()).limit(10)\n",
    "\n",
    "top_10_vertices.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79a7e-ddf7-4224-8aec-f0323c729592",
   "metadata": {},
   "source": [
    "# Machine Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7382b-728b-430a-943f-f8cfd764c525",
   "metadata": {},
   "source": [
    "### Convert the three dealy categories of the Edges dataframe into integers (0,1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bd3a044-73e4-44e4-9218-d877c1d5b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edgs_2 = df_edgs.withColumn(\n",
    "    \"delay_category_int\",\n",
    "    when(col(\"delay_category\") == \"Early\", 0)\n",
    "    .when(col(\"delay_category\") == \"Late\", 1)\n",
    "    .otherwise(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b370c-c841-45df-9d7c-a0ded78b56f1",
   "metadata": {},
   "source": [
    "### Split the data 80% train and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cc927a6-4b9f-4b58-a8a6-c5e6e343e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary function\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Randomly split the data\n",
    "train_data, test_data = df_edgs_2.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626f63c-4d54-47e3-a3fc-dc9c16bf10a3",
   "metadata": {},
   "source": [
    "### Your task is to predict the delay category using any Classifier of your choice.\n",
    "### Prepare your data as needed.\n",
    "### Perform the required features engineering as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba35df15-2dc0-4cff-8e5e-6da98bd70398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Prepare feature columns\n",
    "feature_cols = ['delay', 'distance']\n",
    "\n",
    "# VectorAssembler to combine feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "assembled_train_data = assembler.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "727d611e-0e14-492b-890d-89f285f1a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Define the classifier\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='delay_category_int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1294c-5601-4b27-9075-8623e059a590",
   "metadata": {},
   "source": [
    "### All your steps should be in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d53b451c-9179-4813-8fb4-d68812850bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14ebf3f2-4be1-4076-a182-b585f36ca3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88350706-e4e0-49ec-842c-bff1139eedd1",
   "metadata": {},
   "source": [
    "### Train your model on the trainig data and test on the test data.\n",
    "### You should obtain at least 0.5 f1-score and 0.6 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9b18417-0115-4ce9-bfda-f27f0cb04f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 254:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 1.0\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='delay_category_int', predictionCol='prediction', metricName='f1')\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='delay_category_int', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba3b88-c005-4888-b475-b15f3a69db17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
