{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a919f2-77d5-44d4-9967-19ee12f2952b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718451f8-e4fe-4602-832c-2fd899d407e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951d21e-eca2-40c7-a8e5-879c5bdf0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Read lines from a file stream\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c56f2c2-2e28-4ad7-aead-2112ef9d95ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 10:20:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d69b63-e0f6-4e30-9e00-9feb39ae18b0",
   "metadata": {},
   "source": [
    "#### Define schemas one for each folder of the provided data VertFinalExam and EdgesFinalExam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1742a54-1ec7-485b-ad26-a9de50e93504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfa69ae1-bcf0-4115-98ca-9ff2eaf48f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertSchema = StructType([StructField('id', StringType(), True),\n",
    "                           StructField('City', StringType(), True),\n",
    "                           StructField('State', StringType(), True),\n",
    "                           StructField('Country', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "940725f1-390e-42b6-9cb4-4522e11fb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgSchema = StructType([StructField('tripid', IntegerType(), True),\n",
    "                        StructField('delay', IntegerType(), True),\n",
    "                        StructField('distance', IntegerType(), True),\n",
    "                        StructField('src', StringType(), True),\n",
    "                        StructField('dst', StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114f9ed-7eed-4067-bbaf-0d1afef2d25c",
   "metadata": {},
   "source": [
    "#### Create a streaming reader to read streaming data from the reading sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbdd21f9-0b2b-4512-ada5-95b88d8967f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_df = spark.readStream.format(\"parquet\")\\\n",
    ".schema(vertSchema)\\\n",
    ".load(\"/home/wick/Big_Data/final_project/VertFinalExam/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76cdba44-f9c9-4481-8467-4feee4aee7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vert_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcb79f24-ceac-4688-ada3-513602f4bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edg_df = spark.readStream.format(\"parquet\")\\\n",
    ".schema(edgSchema)\\\n",
    ".load(\"/home/wick/Big_Data/final_project/EdgesFinalExam/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d7df98-6bb4-4704-8da5-1630912f0ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edg_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bff6c3-e00f-4794-adbe-cda342efc0ab",
   "metadata": {},
   "source": [
    "#### For the streaming Edges dataframe create a new column to indicate delay categories as follow:\n",
    "\n",
    "    Early: for early delays (-ve delay values).\n",
    "    Late: for delayed flights (+ve delay values).\n",
    "    OnTime: for on time flights (0 delay values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8b94919-1de8-4211-ba66-1e63051c74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "edg_df1 = edg_df.withColumn(\n",
    "                              \"delay_category\",\n",
    "                              when(col(\"delay\") < 0, \"Early\")\n",
    "                             .when(col(\"delay\") > 0, \"Late\")\n",
    "                             .otherwise(\"OnTime\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2edf394e-3a36-481f-94c9-a5719f43f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripid: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- delay_category: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edg_df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92d54a-607e-4c21-a206-2e46496492a0",
   "metadata": {},
   "source": [
    "#### For the streaming Vertices dataframe remove all rows that contain state as an emplty string state=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7194795f-d746-42a9-85a1-bcdf4a68db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_df1 = vert_df.filter(col(\"State\") != \"\")\n",
    "#vert_df1 = vert_df.where(col(\"State\") != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56340f3b-5e43-4510-8560-c34f38d7755f",
   "metadata": {},
   "source": [
    "#### Create a writer for the final streaming Edges dataframe to write the streaming data in writing sink in a parquet fromat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "128c933b-02f6-465a-9e9c-338cdf22cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edg_writer = edg_df1.writeStream.outputMode(\"append\")\\\n",
    "                .format(\"parquet\")\\\n",
    "                .option(\"path\", \"/home/wick/Big_Data/final_project/first_folder/\")\\\n",
    "                .option(\"checkpointLocation\", \"/home/wick/Big_Data/final_project/edg_chkpnt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01958e20-ec2a-47fe-8e30-1c194c5ebc55",
   "metadata": {},
   "source": [
    "#### Create a writer for the final streaming Vertices dataframe to write the streaming data in writing sink in a parquet fromat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71a88104-1f66-441d-8165-672cd03d60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_writer = vert_df1.writeStream.outputMode(\"append\")\\\n",
    "                .format(\"parquet\")\\\n",
    "                .option(\"path\", \"/home/wick/Big_Data/final_project/second_folder/\")\\\n",
    "                .option(\"checkpointLocation\", \"/home/wick/Big_Data/final_project/vert_chkpnt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e141f1-6215-4bc2-84a3-b92047a5d6df",
   "metadata": {},
   "source": [
    "#### Start a query for the Edges writer. Copy and paste your EdgesFinalExam data to the edges streaming reading source. Wait to make sure the writing sink folder contains all data. Then stop the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2911d89-83cb-463b-b025-54ceaf153f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 23:49:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edg_query = edg_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba7bc21e-7ad2-496e-b851-a0b34e7a090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edg_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4afa2-c874-4e0c-a81d-8f08342573f7",
   "metadata": {},
   "source": [
    "#### Start a query for the Vertices writer. Copy and paste your VertFinalExam data to the vertices streaming reading source. Wait to make sure the writing sink folder contains all data. Then stop the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fe2e293-e8bc-41bf-8225-367eea42d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 23:52:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "vert_query = vert_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07d86675-a007-4581-9096-30e844b1d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875bfa9-ec66-433b-a666-47efaeeb4c0b",
   "metadata": {},
   "source": [
    "#### Using spark.read():\n",
    "\n",
    "    Read the vertices data from the writing sink directory into static Vertices dataframe.\n",
    "    Read the edges data from the writing sink directory into static Edges dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbc89cf-8cf1-4867-b89b-7f44da88c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 10:44:17 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_verts = spark.read.format('parquet')\\\n",
    ".load('/home/wick/Big_Data/final_project/second_folder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b415f4b7-bc58-414c-b68e-d872443d2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edgs = spark.read.format('parquet')\\\n",
    ".load('/home/wick/Big_Data/final_project/first_folder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80e0d6c-2cb5-4b30-a057-30e2e4ea3a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+-------+\n",
      "| id|       City|State|Country|\n",
      "+---+-----------+-----+-------+\n",
      "|ABE|  Allentown|   PA|    USA|\n",
      "|ABI|    Abilene|   TX|    USA|\n",
      "|ABQ|Albuquerque|   NM|    USA|\n",
      "|ABR|   Aberdeen|   SD|    USA|\n",
      "|ABY|     Albany|   GA|    USA|\n",
      "+---+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_verts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e6edbe-64bd-4b18-a026-7ec6f071783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+---+---+--------------+\n",
      "| tripid|delay|distance|src|dst|delay_category|\n",
      "+-------+-----+--------+---+---+--------------+\n",
      "|1010630|  -10|     928|RSW|EWR|         Early|\n",
      "|1021029|   87|     974|RSW|ORD|          Late|\n",
      "|1021346|    0|     928|RSW|EWR|        OnTime|\n",
      "|1021044|   18|     928|RSW|EWR|          Late|\n",
      "|1021730|   29|     748|RSW|IAH|          Late|\n",
      "+-------+-----+--------+---+---+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_edgs.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
